# -*- coding: utf-8 -*-
"""dpre.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/141SxBIPPJswPKrJS1I8eaNcZB4Hkd0s7

dpre.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uqb51TUMSOjDtu8GOrnAgiSJsVO8P7Ox

# **Data Cleaning**
"""

import sys
import pandas as pd

file_path = sys.argv[1]
data = pd.read_csv(file_path)

data.isna().sum()

"""1 Handling Missing Values"""

#Filling the nulls with the mean in the numerical variables
data['Date'] = data['Date'].fillna(data['Date'].mean())
data['CarCount'] = data['CarCount'].fillna(data['CarCount'].mean())
data['BusCount'] = data['BusCount'].fillna(data['BusCount'].mean())
data['Total'] = data['Total'].fillna(data['Total'].mean())

#Filling the nulls with the mode in the categorical variables
data['Traffic Situation'] = data['Traffic Situation'].fillna(data['Traffic Situation'].mode().iloc[0])

data.isna().sum()

"""2 Remove duplicates"""

#number of rows before removing duplicates
print("Number of rows before removing duplicates:", len(data))

# Remove duplicates
data = data.drop_duplicates()

#number of rows after removing duplicates
print("Number of rows after removing duplicates:", len(data))

"""## **Data Transformation**"""

#1 Convert the time column to separate columns for hours, minutes, and seconds
data['Time'] = pd.to_datetime(data['Time'], format='%I:%M:%S %p')
data['Hour'] = data['Time'].dt.hour
data['Minute'] = data['Time'].dt.minute
data['Second'] = data['Time'].dt.second



# Drop the original time column
data = data.drop(columns=['Time'])

"""2 One Hot Encoding"""

data = pd.get_dummies(data, columns=['Day of the week','Traffic Situation'])

"""# **Data Reduction**

1 Feature Selection
"""

from sklearn.feature_selection import VarianceThreshold

# Initialize the VarianceThreshold object with a threshold (e.g., 0.1)
variance_threshold = VarianceThreshold(threshold=0.05)

# Fit and transform the feature matrix to select features
selected_features = variance_threshold.fit_transform(data)

# Get the indices of the selected features
selected_feature_indices = variance_threshold.get_support(indices=True)

# Update the data with only the selected features
data = data.iloc[:, selected_feature_indices]

data

"""2 Dimensionality reduction"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data)

"""# **Data Discretization**

1 Discretize "CarCount" into 4 age groups
"""

# Define the intervals for discretization
bins = [0, 30, 60, 90, float('inf')]

# Define the labels for the categories
labels = ['Low', 'Medium', 'High', 'Very High']

# Discretize the 'CarCount' column
data['CarCount_Category'] = pd.cut(data['CarCount'], bins=bins, labels=labels, right=False)

# Display the discretized data
print(data[['CarCount', 'CarCount_Category']])

"""2"""

# Define the number of bins for discretization
num_bins = 5  # You can adjust the number of bins as per your requirements

# Perform equal width binning
bin_width = (data['Total'].max() - data['Total'].min()) / num_bins
bins = [data['Total'].min() + i * bin_width for i in range(num_bins + 1)]
labels = [f'Bin {i+1}' for i in range(num_bins)]

# Discretize the 'Total' column
data['Total_Category'] = pd.cut(data['Total'], bins=bins, labels=labels, right=False)

"""Display the discretized data
print(data[['Total', 'Total_Category']])
"""

data.to_csv("res_dpre.csv",index=False)

import subprocess
subprocess.run(["python3", "eda.py", "traffic.csv"])